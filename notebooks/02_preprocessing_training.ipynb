{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/ilapak3/train.csv\")\n",
    "df['times'] = pd.to_datetime(df['times'])\n",
    "df = df.sort_values(by='times').reset_index(drop=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2879ad1",
   "metadata": {},
   "source": [
    "# Feature Engineering & Scalling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc144f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times\n",
    "df[\"day\"] = df[\"times\"].dt.day\n",
    "df[\"hour\"] = df[\"times\"].dt.hour\n",
    "df[\"minute\"] = df[\"times\"].dt.minute\n",
    "\n",
    "# Diff of sealing\n",
    "df[\"diff_sealing_vertical\"] = df[\"Suhu Sealing Vertical Atas (oC)\"] - df[\"Suhu Sealing Vertikal Bawah (oC)\"]\n",
    "df[\"diff_sealing_horizontal\"] = df[\"Suhu Sealing Horizontal Depan/Kanan (oC)\"] - df[\"Suhu Sealing Horizontal Belakang/Kiri (oC )\"]\n",
    "\n",
    "# Diff of output reject\n",
    "df[\"diff_output\"] = df[\"Counter Output (pack)\"] - df[\"Counter Reject (pack)\"]\n",
    "\n",
    "# Diff of output before\n",
    "df[\"delta_output\"] = df[\"Counter Output (pack)\"].diff(periods=5)\n",
    "df[\"delta_reject\"] = df[\"Counter Reject (pack)\"].diff(periods=5)\n",
    "\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6997954",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68265fbc",
   "metadata": {},
   "source": [
    "## Scale for Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ea23e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_cols = [\n",
    "    \"Suhu Sealing Vertikal Bawah (oC)\",\n",
    "    \"Suhu Sealing Vertical Atas (oC)\",\n",
    "    \"Suhu Sealing Horizontal Depan/Kanan (oC)\",\n",
    "    \"Suhu Sealing Horizontal Belakang/Kiri (oC )\",\n",
    "    \"diff_sealing_vertical\",\n",
    "    \"diff_sealing_horizontal\",\n",
    "    \"diff_output\",\n",
    "    \"delta_output\",\n",
    "    \"delta_reject\",\n",
    "    \"Counter Output (pack)\",\n",
    "    \"Counter Reject (pack)\",\n",
    "    \"Availability(%)\",\n",
    "    \"Performance(%)\",\n",
    "    \"Quality(%)\",\n",
    "    \"OEE(%)\",\n",
    "    \"Speed(rpm)\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"minute\",\n",
    "    'Downtime_sec',\n",
    "    'Output Time_sec', 'Total Time_sec'\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Status\",\n",
    "    \"Shift\",\n",
    "    'Jaws Position',\n",
    "       'Doser Drive Enable', 'Sealing Enable', 'Machine Alarm'\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4abd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Condition\"])\n",
    "y = df[\"Condition\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", MinMaxScaler(), continuous_cols),\n",
    "        (\"cat\", \"passthrough\", categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_all = preprocessor.fit_transform(X)\n",
    "y_all = y\n",
    "\n",
    "# Apply SMOTE\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf9355",
   "metadata": {},
   "source": [
    "# Sliding Window Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2c1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sliding_windows(X, y, window_size):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - window_size + 1):\n",
    "        X_seq.append(X[i:i+window_size])\n",
    "        y_seq.append(y[i+window_size-1])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "window_size = 10\n",
    "X_seq, y_seq = create_sliding_windows(X_all, y_all, window_size)\n",
    "\n",
    "print(f\"X Sequence Shape: {X_seq.shape}\")\n",
    "print(f\"y Sequence Shape: {y_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea12e8",
   "metadata": {},
   "source": [
    "# Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=10)\n",
    "for train_index, val_index in tscv.split(X_seq):\n",
    "    X_train, X_val = X_seq[train_index], X_seq[val_index]\n",
    "    y_train, y_val = y_seq[train_index], y_seq[val_index]\n",
    "\n",
    "print(f\"X Train Shape: {X_train.shape}\")\n",
    "print(f\"y Train Shape: {y_train.shape}\")\n",
    "print(f\"X Val Shape: {X_val.shape}\")\n",
    "print(f\"y Val Shape: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13be106d",
   "metadata": {},
   "source": [
    "# Build & Train BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a928c",
   "metadata": {},
   "source": [
    "## Define Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63e53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlotTraining(Callback):\n",
    "    def __init__(self, interval=10):\n",
    "        self.interval = interval\n",
    "        self.history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        for key in self.history:\n",
    "            if key in logs:\n",
    "                self.history[key].append(logs[key])\n",
    "        \n",
    "        if (epoch + 1) % self.interval == 0:\n",
    "            self.plot_metrics(epoch)\n",
    "\n",
    "    def plot_metrics(self, epoch):\n",
    "        epochs = range(1, epoch + 2)\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        # Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.history['loss'], label='Train Loss')\n",
    "        plt.plot(epochs, self.history['val_loss'], label='Val Loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        # Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.history['accuracy'], label='Train Acc')\n",
    "        plt.plot(epochs, self.history['val_accuracy'], label='Val Acc')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea6f25",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91bc369",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 64\n",
    "INPUT_SHAPE = (X_train.shape[1], X_train.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40400671",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1267b9",
   "metadata": {},
   "source": [
    "# Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e91447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(X, y, batch_size=32, shuffle=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=10000)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_ds = create_dataset(X_val, y_val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be296cff",
   "metadata": {},
   "source": [
    "## Arhictecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf656e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv1D(filters=16, kernel_size=3, activation='relu', padding='same',\n",
    "           kernel_regularizer=l2(1e-2), input_shape=INPUT_SHAPE),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', padding='same',\n",
    "           kernel_regularizer=l2(1e-2\n",
    "           )),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    MaxPooling1D(pool_size=2),\n",
    "\n",
    "    Bidirectional(LSTM(128, return_sequences=False, dropout=0.4,\n",
    "                       kernel_regularizer=l2(1e-4))),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(64, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(32, activation='relu', kernel_regularizer=l2(1e-4)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d00ba",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa11709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(alpha=0.25, gamma=2.0):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "        \n",
    "        # Convert to one-hot\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_true = tf.one_hot(y_true, depth=3)\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        # Calculate focal loss\n",
    "        ce = -y_true * tf.math.log(y_pred)\n",
    "        weight = alpha * y_true * tf.pow((1 - y_pred), gamma)\n",
    "        fl = weight * ce\n",
    "        reduced_fl = tf.reduce_sum(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    \n",
    "    return focal_loss_fixed\n",
    "\n",
    "optimizer = AdamW(learning_rate=0.003)\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),\n",
    "    PlotTraining(interval=10)\n",
    "]\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=focal_loss(alpha=0.25, gamma=3.0),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c573d69",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a4930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(X_val)\n",
    "y_pred = y_pred_prob.argmax(axis=1)\n",
    "\n",
    "print(classification_report(y_val, y_pred, target_names=[\"normal\", \"warning\", \"leak\"]))\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred)\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=[\"normal\", \"warning\", \"leak\"], yticklabels=[\"normal\", \"warning\", \"leak\"], ax=axs[0])\n",
    "axs[0].set_title('Confusion Matrix (Counts)')\n",
    "axs[0].set_xlabel('Predicted')\n",
    "axs[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(cm_percent * 100, annot=True, fmt='.1f', cmap='Greens', xticklabels=[\"normal\", \"warning\", \"leak\"], yticklabels=[\"normal\", \"warning\", \"leak\"], ax=axs[1])\n",
    "axs[1].set_title('Confusion Matrix (%)')\n",
    "axs[1].set_xlabel('Predicted')\n",
    "axs[1].set_ylabel('Actual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65991dee",
   "metadata": {},
   "source": [
    "# Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda923a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_condition(new_df, feature_cols_cat, feature_cols_cont, scaler, model, window_size=5):\n",
    "    new_df = new_df.sort_values(by='times')\n",
    "    assert len(new_df) >= window_size, \"Butuh minimal window_size data\"\n",
    "\n",
    "    X_cat = new_df[feature_cols_cat].values\n",
    "    X_cont = scaler.transform(new_df[feature_cols_cont])\n",
    "    X_input = np.hstack([X_cat, X_cont])[-window_size:]\n",
    "    X_input = X_input.reshape(1, window_size, -1)\n",
    "\n",
    "    pred_probs = model.predict(X_input)\n",
    "    pred_class = pred_probs.argmax(axis=1)[0]\n",
    "    return pred_class, pred_probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d74a9c0",
   "metadata": {},
   "source": [
    "# Save Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee8b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import save_model\n",
    "# import joblib\n",
    "\n",
    "# save_model(model, \"bilstm_model.h5\")\n",
    "# joblib.dump(scaler, \"scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
