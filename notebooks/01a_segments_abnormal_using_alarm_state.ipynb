{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eb82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb046f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7760fb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datalog_ilapak10.csv\", index_col=0, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c0a607",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c126f3d",
   "metadata": {},
   "source": [
    "## I. Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1faf60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'suhu_sealing_vertikal_bawah', \n",
    "    'suhu_sealing_vertikal_atas',\n",
    "    'suhu_sealing_horizontal_depan',\n",
    "    'suhu_sealing_horizontal_belakang',\n",
    "    'downtime_sec',\n",
    "    'output_time_sec',\n",
    "    'total_time_sec'\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(16, 10))\n",
    "plt.suptitle(\"Outlier Analysis for Numeric Features\", fontsize=18, weight='bold', y=1.02)\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(y=df[col], color=sns.color_palette(\"pastel\")[0])\n",
    "    plt.title(col.replace('_', ' ').title(), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495dda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()\n",
    "\n",
    "numeric_cols = [\n",
    "    'suhu_sealing_vertikal_bawah', \n",
    "    'suhu_sealing_vertikal_atas',\n",
    "    'suhu_sealing_horizontal_depan',\n",
    "    'suhu_sealing_horizontal_belakang',\n",
    "    'downtime_sec',\n",
    "    'output_time_sec',\n",
    "    'total_time_sec'\n",
    "]\n",
    "\n",
    "print(f\"Shape before outlier removal: {df_cleaned.shape}\")\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_cleaned[col].quantile(0.25)\n",
    "    Q3 = df_cleaned[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_cleaned = df_cleaned[(df_cleaned[col] >= lower_bound) & (df_cleaned[col] <= upper_bound)]\n",
    "\n",
    "print(f\"Shape after outlier removal: {df_cleaned.shape}\")\n",
    "print(f\"Percentage of data removed: {1 - df_cleaned.shape[0] / df.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.suptitle(\"Numeric Features after Outlier Removed\", fontsize=18, weight='bold', y=1.02)\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(y=df_cleaned[col], color=sns.color_palette(\"pastel\")[0])\n",
    "    plt.title(col.replace('_', ' ').title(), fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7e0e3",
   "metadata": {},
   "source": [
    "## II. Distribution of Binary Operational Signals\n",
    "\n",
    "The following plot illustrates the distribution of binary operational columns such as `jaws_position`, `knife_position`, `pump_position_stop`, `doser_drive_enable`, `sealing_enable`, and `machine_alarm`. These features represent the status (ON/OFF or Active/Inactive) of various mechanical components in the packaging process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = [\n",
    "    'jaws_position', 'knife_position', 'pump_position_stop',\n",
    "    'doser_drive_enable', 'sealing_enable', 'machine_alarm'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(binary_cols):\n",
    "    ax = axes[i]\n",
    "    sns.countplot(data=df_cleaned, x=col, ax=ax, palette='pastel')\n",
    "\n",
    "    total = len(df_cleaned)\n",
    "    for p in ax.patches:\n",
    "        count = p.get_height()\n",
    "        percentage = f'{100 * count / total:.1f}%'\n",
    "        x = p.get_x() + p.get_width() / 2\n",
    "        y = count\n",
    "        ax.text(x, y + total * 0.01, percentage, ha='center', fontsize=11)\n",
    "\n",
    "    ax.set_title(f'Distribution of {col.replace(\"_\", \" \").title()}', fontsize=13)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle('Binary Signal Distributions (Operational Statuses)', fontsize=16, weight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e4d9f",
   "metadata": {},
   "source": [
    "### Binary Signal Analysis for LSTM AE Preparation\n",
    "The visualization illustrates the distribution of binary operational signals from the datalog_ilapak10 dataset. The key findings are as follows:\n",
    "| Feature              | Dominant Label | Proportion | Insight                                                   |\n",
    "| -------------------- | -------------- | ---------- | --------------------------------------------------------- |\n",
    "| `jaws_position`      | 0              | 100.0%     | No variation, always in position 0 — not informative      |\n",
    "| `knife_position`     | 0              | 100.0%     | Constant at 0, suggesting no operational switching        |\n",
    "| `pump_position_stop` | 0              | 100.0%     | No observed activation, redundant for modeling            |\n",
    "| `doser_drive_enable` | 0              | 71.1%      | Predominantly inactive, but contains valid signal changes |\n",
    "| `sealing_enable`     | 0              | 68.4%      | Shows both active and inactive states                     |\n",
    "| `machine_alarm`      | 0              | 67.4%      | Around one-third of data indicates alarm conditions       |\n",
    "\n",
    "**Interpretation:**\n",
    "- Features with no variance (jaws_position, knife_position, pump_position_stop) offer no learning value and should be excluded from model training.\n",
    "- The remaining binary features exhibit meaningful variability, potentially capturing machine operational states.\n",
    "- Most notably, machine_alarm serves as a proxy for identifying faulty conditions.\n",
    "\n",
    "**Conclusion for LSTM Autoencoder (LSTM AE) Training:**\n",
    "\n",
    "Since an LSTM Autoencoder should learn only from normal (non-anomalous) behavior, it is essential to filter the training data accordingly. Based on the machine_alarm feature, data labeled with machine_alarm == 0 represents normal operation and is suitable for training the LSTM AE:\n",
    "\n",
    "```python\n",
    "df_normal = df[df[\"machine_alarm\"] == 0]\n",
    "```\n",
    "This approach ensures the model is exposed exclusively to baseline behavior. During deployment, the model can then flag unseen patterns with high reconstruction error as potential anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb82fcbe",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34faea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(df_cleaned.corr(), annot=True, cmap=\"coolwarm\", cbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ad690",
   "metadata": {},
   "source": [
    "## III. Trend Analysis of Sealing Temperature Columns During Abnormal Machine States\n",
    "To gain insight into how temperature behavior correlates with abnormal machine states, we analyze the time-series trend of all sealing-related temperature columns:\n",
    "\n",
    "- suhu_sealing_vertikal_bawah\n",
    "- suhu_sealing_vertikal_atas\n",
    "- suhu_sealing_horizontal_depan\n",
    "- suhu_sealing_horizontal_belakang\n",
    "\n",
    "This analysis is segmented based on the machine_alarm flag to highlight deviations during non-zero (abnormal) conditions.\n",
    "\n",
    "**Objectives:**\n",
    "- Identify any temperature spikes, fluctuations, or irregularities that correspond with machine alarms.\n",
    "- Validate whether thermal anomalies could be potential early indicators of machine faults.\n",
    "\n",
    "**Visualization Strategy:**\n",
    "- Plot time-series overlays for each temperature column.\n",
    "- Overlay or color regions where machine_alarm != 0 to highlight abnormal segments.\n",
    "\n",
    "**Why This Matters:**\n",
    "\n",
    "Temperature stability is crucial in packaging processes involving sealing. Any inconsistency or drift may result in sealing failures or machine alarms. By observing how these signals behave during alarm conditions, we can:\n",
    "\n",
    "- Understand root causes of anomalies.\n",
    "- Improve feature selection for anomaly detection models.\n",
    "- Potentially predict alarms before they occur using pre-alarm temperature patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49275f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function helper\n",
    "def get_abnormal_segments(mask):\n",
    "    segments = []\n",
    "    start = None\n",
    "    for i, val in enumerate(mask):\n",
    "        if val and start is None:\n",
    "            start = i\n",
    "        elif not val and start is not None:\n",
    "            segments.append((start, i - 1))\n",
    "            start = None\n",
    "    if start is not None:\n",
    "        segments.append((start, len(mask) - 1))\n",
    "    return segments\n",
    "\n",
    "def plot_temperature(df: pd.DataFrame, period: str = \"all_time\", with_segments: bool = False):\n",
    "    temp_cols = [\n",
    "        'suhu_sealing_vertikal_bawah', 'suhu_sealing_vertikal_atas',\n",
    "        'suhu_sealing_horizontal_depan', 'suhu_sealing_horizontal_belakang'\n",
    "    ]\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Index of dataframe must be a DatatimeIndex\")\n",
    "\n",
    "    now = df.index.max()\n",
    "    if period == \"last_day\":\n",
    "        df_period = df[df.index >= now - pd.Timedelta(days=1)]\n",
    "    elif period == \"last_week\":\n",
    "        df_period = df[df.index >= now - pd.Timedelta(days=7)]\n",
    "    elif period == \"last_month\":\n",
    "        df_period = df[df.index >= now - pd.Timedelta(days=30)]\n",
    "    elif period == \"last_year\":\n",
    "        df_period = df[df.index >= now - pd.Timedelta(days=365)]\n",
    "    else:\n",
    "        df_period = df\n",
    "    \n",
    "    _, axes = plt.subplots(len(temp_cols), 1, figsize=(18, 10), sharex=True)\n",
    "    for i, col in enumerate(temp_cols):\n",
    "        sns.lineplot(data=df_period, x=df_period.index, y=col, ax=axes[i], palette=\"pastel\")\n",
    "        axes[i].set_title(f\"{col.replace('_', ' ').title()}\", fontsize=12, loc=\"left\")\n",
    "        axes[i].set_ylabel(\"Temp (°C)\")\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "        if with_segments:\n",
    "            abnormal_mask = (\n",
    "                (df_period[\"machine_alarm\"] != 0) &\n",
    "                (df_period[\"doser_drive_enable\"] != 0) &\n",
    "                (df_period[\"sealing_enable\"] != 0)\n",
    "            )\n",
    "            segments = get_abnormal_segments(abnormal_mask.values)\n",
    "            for start, end in segments:\n",
    "                axes[i].axvspan(df_period.index[start], df_period.index[end], color='red', alpha=0.3)\n",
    "    \n",
    "    axes[-1].set_xlabel(\"Timestamp\")\n",
    "    highlight_text = \"with Machine Alarm Highlighted\" if with_segments else \"\"\n",
    "    plt.suptitle(f\"Sealing Temperature Trends {highlight_text} ({period.replace('_', ' ').title()})\",\n",
    "                 fontsize=16, weight=\"bold\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b711b0bd",
   "metadata": {},
   "source": [
    "### All Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcbda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temperature(df_cleaned, with_segments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467a486",
   "metadata": {},
   "source": [
    "### Last Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e31906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temperature(df_cleaned, period=\"last_month\", with_segments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efde80",
   "metadata": {},
   "source": [
    "### Last Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7416b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temperature(df_cleaned, period=\"last_week\", with_segments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45132f5",
   "metadata": {},
   "source": [
    "### Last Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e255beb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_temperature(df_cleaned, period=\"last_day\", with_segments=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478563d0",
   "metadata": {},
   "source": [
    "## IV. Data Normal Temperature Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normal = df_cleaned[(df_cleaned[\"machine_alarm\"] == 0) & (df_cleaned[\"doser_drive_enable\"] == 0) & (df_cleaned[\"sealing_enable\"] == 0)]\n",
    "plot_temperature(data_normal, period=\"last_day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170708bb",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be6ea33",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d76b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_normal[numeric_cols] = scaler.fit_transform(data_normal[numeric_cols])\n",
    "data_normal.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de01e1cc",
   "metadata": {},
   "source": [
    "### Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b05c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop that features because they are not useful (just 0)\n",
    "df_final = data_normal.drop(columns=[\"jaws_position\", \"knife_position\", \"pump_position_stop\"], axis=1)\n",
    "df_final.to_csv(\"datalog_ilapak10_normal.csv\")\n",
    "df_final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5dada",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac209b9",
   "metadata": {},
   "source": [
    "### Create Sequences & Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data: pd.DataFrame, window_size: int, stride: int = 5) -> np.ndarray:\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - window_size, stride):\n",
    "        sequences.append(data[i:i + window_size])\n",
    "    return np.array(sequences)\n",
    "\n",
    "sequences = create_sequences(df_final, window_size=30, stride=5)\n",
    "print(f\"Sequences Shape: {sequences.shape}\")\n",
    "\n",
    "train_size = int(0.8 * len(sequences))\n",
    "X_train = sequences[:train_size]\n",
    "X_val = sequences[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209d1f6",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559b01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "\n",
    "class LSTMAutoEncoder(Model):\n",
    "    def __init__(self, window_size: int, n_features: int):\n",
    "        super(LSTMAutoEncoder, self).__init__()\n",
    "        self.encoder_layer1 = LSTM(64, return_sequences=True, activation='relu')\n",
    "        self.encoder_layer2 = LSTM(32, return_sequences=False, activation='relu')\n",
    "        self.repeat_vector = RepeatVector(window_size)\n",
    "        self.decoder_layer1 = LSTM(32, return_sequences=True, activation='relu')\n",
    "        self.decoder_layer2 = LSTM(64, return_sequences=True, activation='relu')\n",
    "        self.output_layer = TimeDistributed(Dense(n_features))\n",
    "\n",
    "    def call(self, inputs, training = False):\n",
    "        x = self.encoder_layer1(inputs)\n",
    "        x = self.encoder_layer2(x)\n",
    "        x = self.repeat_vector(x)\n",
    "        x = self.decoder_layer1(x)\n",
    "        x = self.decoder_layer2(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482647e2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fd8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, Callback, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "from typing import Tuple, List, Union, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd85f9e",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989426ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Handles data preprocessing for time series anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 30, stride: int = 5):\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def create_sequences(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Create sliding window sequences from time series data\"\"\"\n",
    "        sequences = []\n",
    "        for i in range(0, len(data) - self.window_size + 1, self.stride):\n",
    "            sequences.append(data.iloc[i:i + self.window_size].values)\n",
    "        return np.array(sequences)\n",
    "    \n",
    "    def fit_scaler(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fit scaler on training data and transform\"\"\"\n",
    "        scaled_data = self.scaler.fit_transform(data)\n",
    "        return pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
    "    \n",
    "    def transform(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Transform data using fitted scaler\"\"\"\n",
    "        if self.scaler is None:\n",
    "            raise ValueError(\"Scaler not fitted. Call fit_scaler first.\")\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return pd.DataFrame(scaled_data, columns=data.columns, index=data.index)\n",
    "    \n",
    "    def split_sequences(self, sequences: np.ndarray, train_size: float = 0.7, val_size: float = 0.15) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Split sequences into train, validation, and test sets\"\"\"\n",
    "        total_len = len(sequences)\n",
    "        train_end = int(train_size * total_len)\n",
    "        val_end = train_end + int(val_size * total_len)\n",
    "\n",
    "        return sequences[:train_end], sequences[train_end:val_end], sequences[val_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52d638",
   "metadata": {},
   "source": [
    "#### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f541943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoEncoder(Model):\n",
    "    \"\"\"LSTM-based Autoencoder for anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int, n_features: int, \n",
    "                 encoder_units: List[int] = [64, 32], \n",
    "                 decoder_units: List[int] = [32, 64]):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_lstm1 = LSTM(encoder_units[0], return_sequences=True, activation='relu')\n",
    "        self.encoder_lstm2 = LSTM(encoder_units[1], return_sequences=False, activation='relu')\n",
    "\n",
    "        # Decoder\n",
    "        self.repeat_vector = RepeatVector(window_size)\n",
    "        self.decoder_lstm1 = LSTM(decoder_units[0], return_sequences=True, activation='relu')\n",
    "        self.decoder_lstm2 = LSTM(decoder_units[1], return_sequences=True, activation='relu')\n",
    "        self.output_layer = TimeDistributed(Dense(n_features))\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Encoder\n",
    "        x = self.encoder_lstm1(inputs, training=training)\n",
    "        encoded = self.encoder_lstm2(x, training=training)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.repeat_vector(encoded)\n",
    "        x = self.decoder_lstm1(x, training=training)\n",
    "        x = self.decoder_lstm2(x, training=training)\n",
    "        decoded = self.output_layer(x)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385dae8",
   "metadata": {},
   "source": [
    "#### Callback Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc973909",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCallback(Callback):\n",
    "    \"\"\"callback for monitoring training progress\"\"\"\n",
    "    \n",
    "    def __init__(self, plot_interval: int = 10, save_plots: bool = False):\n",
    "        super().__init__()\n",
    "        self.plot_interval = plot_interval\n",
    "        self.save_plots = save_plots\n",
    "        if save_plots:\n",
    "            os.makedirs(\"training_plots\", exist_ok=True)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.plot_interval == 0:\n",
    "            self._plot_progress(epoch, logs)\n",
    "    \n",
    "    def _plot_progress(self, epoch, logs):\n",
    "        \"\"\"Plot training progress\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Get history from model\n",
    "        history = self.model.history.history\n",
    "        \n",
    "        plt.plot(history['loss'], label='Train Loss')\n",
    "        plt.plot(history['val_loss'], label='Val Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Training Progress - Epoch {epoch+1}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if self.save_plots:\n",
    "            plt.savefig(f\"training_plots/progress_epoch_{epoch+1}.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ab6d4",
   "metadata": {},
   "source": [
    "#### Orchestrator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce516be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetector:\n",
    "    \"\"\"Main class for LSTM Autoencoder-based anomaly detection\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size: int = 30, stride: int = 5):\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride\n",
    "        self.preprocessor = DataPreprocessor(window_size, stride)\n",
    "        self.model = None\n",
    "        self.threshold = None\n",
    "        self.history = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def build_model(self, n_features: int, encoder_units: List[int] = [64, 32], \n",
    "                   decoder_units: List[int] = [32, 64]):\n",
    "        \"\"\"Build and compile the LSTM autoencoder model\"\"\"\n",
    "        self.model = LSTMAutoEncoder(\n",
    "            window_size=self.window_size,\n",
    "            n_features=n_features,\n",
    "            encoder_units=encoder_units,\n",
    "            decoder_units=decoder_units\n",
    "        )\n",
    "        self.model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n",
    "        \n",
    "    def prepare_data(self, data: pd.DataFrame, fit_scaler: bool = True) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Prepare data for training\"\"\"\n",
    "        # Scale data\n",
    "        if fit_scaler:\n",
    "            scaled_data = self.preprocessor.fit_scaler(data)\n",
    "        else:\n",
    "            scaled_data = self.preprocessor.transform(data)\n",
    "        \n",
    "        # Create sequences\n",
    "        sequences = self.preprocessor.create_sequences(scaled_data)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_val, X_test = self.preprocessor.split_sequences(sequences)\n",
    "        \n",
    "        return X_train, X_val, X_test\n",
    "        \n",
    "    def train(self, X_train: np.ndarray, X_val: np.ndarray, \n",
    "              epochs: int = 50, batch_size: int = 32, patience: int = 10,\n",
    "              save_best: bool = True, plot_progress: bool = True) -> tf.keras.callbacks.History:\n",
    "        \"\"\"Train the autoencoder model\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not built. Call build_model() first.\")\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "        ]\n",
    "        \n",
    "        if save_best:\n",
    "            callbacks.append(\n",
    "                ModelCheckpoint(\"best_model.keras\", monitor='val_loss', \n",
    "                              save_best_only=True, verbose=1)\n",
    "            )\n",
    "            \n",
    "        if plot_progress:\n",
    "            callbacks.append(TrainingCallback(plot_interval=max(1, epochs//5)))\n",
    "        \n",
    "        print(f\"Training model with {len(X_train)} training samples...\")\n",
    "        self.history = self.model.fit(\n",
    "            X_train, X_train,\n",
    "            validation_data=(X_val, X_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def calculate_threshold(self, X_train: np.ndarray, percentile: float = 95) -> float:\n",
    "        \"\"\"Calculate anomaly detection threshold from training data\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Train model first.\")\n",
    "            \n",
    "        print(\"Calculating threshold from training data...\")\n",
    "        train_pred = self.model.predict(X_train, verbose=0)\n",
    "        train_errors = self._calculate_errors(X_train, train_pred)\n",
    "        \n",
    "        self.threshold = np.percentile(train_errors, percentile)\n",
    "        \n",
    "        print(f\"Threshold set to: {self.threshold:.6f}\")\n",
    "        print(f\"Training errors - Mean: {np.mean(train_errors):.6f}, Std: {np.std(train_errors):.6f}\")\n",
    "        \n",
    "        return self.threshold\n",
    "    \n",
    "    def _calculate_errors(self, original: np.ndarray, reconstructed: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate reconstruction errors\"\"\"\n",
    "        return np.array([\n",
    "            mean_squared_error(orig.flatten(), recon.flatten())\n",
    "            for orig, recon in zip(original, reconstructed)\n",
    "        ])\n",
    "    \n",
    "    def predict(self, X: np.ndarray, return_details: bool = False) -> Union[np.ndarray, Dict]:\n",
    "        \"\"\"Predict anomalies in new data\"\"\"\n",
    "        if self.model is None or self.threshold is None:\n",
    "            raise ValueError(\"Model not trained or threshold not set.\")\n",
    "        \n",
    "        predictions = self.model.predict(X, verbose=0)\n",
    "        errors = self._calculate_errors(X, predictions)\n",
    "        anomalies = errors > self.threshold\n",
    "        \n",
    "        if not return_details:\n",
    "            return anomalies\n",
    "            \n",
    "        return {\n",
    "            'anomalies': anomalies,\n",
    "            'errors': errors,\n",
    "            'threshold': self.threshold,\n",
    "            'num_anomalies': np.sum(anomalies),\n",
    "            'anomaly_rate': np.mean(anomalies),\n",
    "            'max_error': np.max(errors),\n",
    "            'mean_error': np.mean(errors)\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self, X: np.ndarray, title: str = \"Anomaly Detection Results\"):\n",
    "        \"\"\"Visualize reconstruction errors and anomalies\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained.\")\n",
    "            \n",
    "        predictions = self.model.predict(X, verbose=0)\n",
    "        errors = self._calculate_errors(X, predictions)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "        \n",
    "        # Error timeline\n",
    "        axes[0, 0].plot(errors, alpha=0.7)\n",
    "        if self.threshold:\n",
    "            axes[0, 0].axhline(y=self.threshold, color='red', linestyle='--', label='Threshold')\n",
    "            anomalies = errors > self.threshold\n",
    "            axes[0, 0].scatter(np.where(anomalies)[0], errors[anomalies], \n",
    "                             color='red', s=20, alpha=0.8, label='Anomalies')\n",
    "        axes[0, 0].set_title('Reconstruction Errors')\n",
    "        axes[0, 0].set_xlabel('Sample')\n",
    "        axes[0, 0].set_ylabel('Error')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Error distribution\n",
    "        axes[0, 1].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "        if self.threshold:\n",
    "            axes[0, 1].axvline(x=self.threshold, color='red', linestyle='--', label='Threshold')\n",
    "        axes[0, 1].set_title('Error Distribution')\n",
    "        axes[0, 1].set_xlabel('Error')\n",
    "        axes[0, 1].set_ylabel('Frequency')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        axes[1, 0].boxplot([errors], labels=['Errors'])\n",
    "        if self.threshold:\n",
    "            axes[1, 0].axhline(y=self.threshold, color='red', linestyle='--', label='Threshold')\n",
    "        axes[1, 0].set_title('Error Statistics')\n",
    "        axes[1, 0].set_ylabel('Error')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cumulative distribution\n",
    "        sorted_errors = np.sort(errors)\n",
    "        cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "        axes[1, 1].plot(sorted_errors, cumulative)\n",
    "        if self.threshold:\n",
    "            axes[1, 1].axvline(x=self.threshold, color='red', linestyle='--', label='Threshold')\n",
    "        axes[1, 1].set_title('Cumulative Distribution')\n",
    "        axes[1, 1].set_xlabel('Error')\n",
    "        axes[1, 1].set_ylabel('Cumulative Probability')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n=== {title} Summary ===\")\n",
    "        print(f\"Samples: {len(errors)}\")\n",
    "        print(f\"Mean error: {np.mean(errors):.6f}\")\n",
    "        print(f\"Std error: {np.std(errors):.6f}\")\n",
    "        if self.threshold:\n",
    "            anomalies = errors > self.threshold\n",
    "            print(f\"Anomalies: {np.sum(anomalies)} ({np.mean(anomalies)*100:.2f}%)\")\n",
    "    \n",
    "    def save_model(self, filepath: str = \"anomaly_detector.keras\"):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"No model to save.\")\n",
    "        self.model.save(filepath)\n",
    "        \n",
    "        # Save additional parameters\n",
    "        params = {\n",
    "            'window_size': self.window_size,\n",
    "            'stride': self.stride,\n",
    "            'threshold': self.threshold,\n",
    "            'scaler': self.preprocessor.scaler\n",
    "        }\n",
    "        joblib.dump(params, filepath.replace('.keras', '_params.pkl'))\n",
    "        print(f\"Model and parameters saved to {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath: str):\n",
    "        \"\"\"Load a saved model\"\"\"\n",
    "        self.model = tf.keras.models.load_model(filepath)\n",
    "        \n",
    "        # Load additional parameters\n",
    "        params_file = filepath.replace('.keras', '_params.pkl')\n",
    "        if os.path.exists(params_file):\n",
    "            params = joblib.load(params_file)\n",
    "            self.window_size = params['window_size']\n",
    "            self.stride = params['stride']\n",
    "            self.threshold = params['threshold']\n",
    "            self.preprocessor.scaler = params['scaler']\n",
    "        \n",
    "        print(f\"Model loaded from {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = AnomalyDetector()\n",
    "# Preparing data\n",
    "print(f\"Preparing data...\")\n",
    "X_train, X_val, X_test = pipeline.prepare_data(df_final, fit_scaler=True)\n",
    "print(f\"Traing Sequences: {X_train.shape}\")\n",
    "print(f\"Validation Sequences: {X_val.shape}\")\n",
    "print(f\"Test Sequences: {X_test.shape}\\n\")\n",
    "\n",
    "# Build model\n",
    "print(f\"Building model...\")\n",
    "pipeline.build_model(n_features=df_final.shape[1], encoder_units=[64, 32], decoder_units=[32, 64])\n",
    "print(f\"Model built sucessfully.\\n\")\n",
    "\n",
    "# Train model\n",
    "print(f\"Training model...\")\n",
    "history = pipeline.train(X_train, X_val, epochs=50, batch_size=32, patience=10, save_best=True, plot_progress=True)\n",
    "print(f\"Model trained sucessfully.\\n\")\n",
    "\n",
    "# Calculate threshold\n",
    "print(f\"Calculating threshold...\")\n",
    "threshold = pipeline.calculate_threshold(X_train, percentile=95)\n",
    "print(f\"Threshold calculated sucessfully.\\n\")\n",
    "\n",
    "# Test on validation data\n",
    "print(f\"Testing model on validation data...\")\n",
    "val_results = pipeline.predict(X_val, return_details=True)\n",
    "print(f\"Validation anomalies detected: {val_results['num_anomalies']} ({val_results['anomaly_rate']*100:.2f}%)\")\n",
    "\n",
    "pipeline.visualize_results(X_val, \"Validation Data Results\")\n",
    "\n",
    "print(\"Testing on test data...\")\n",
    "test_results = pipeline.predict(X_test, return_details=True)\n",
    "print(f\"Test anomalies detected: {test_results['num_anomalies']} ({test_results['anomaly_rate']*100:.2f}%)\")\n",
    "pipeline.visualize_results(X_test, \"Test Data Results\")\n",
    "\n",
    "print(\"\\nSaving model...\")\n",
    "pipeline.save_model(\"trained_anomaly_detector.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
